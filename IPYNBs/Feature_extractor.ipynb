{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c91022f4",
   "metadata": {},
   "source": [
    "Step 1:- Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8733eef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T03:07:24.748360Z",
     "iopub.status.busy": "2023-03-09T03:07:24.747987Z",
     "iopub.status.idle": "2023-03-09T03:07:24.759123Z",
     "shell.execute_reply": "2023-03-09T03:07:24.757961Z",
     "shell.execute_reply.started": "2023-03-09T03:07:24.748327Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import array\n",
    "from pickle import load\n",
    "\n",
    "from PIL import Image\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys, time, os, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, BatchNormalization\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from pathlib import Path\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b916365",
   "metadata": {},
   "source": [
    "Step 2:- Data loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2988f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T03:07:24.765000Z",
     "iopub.status.busy": "2023-03-09T03:07:24.764724Z",
     "iopub.status.idle": "2023-03-09T03:07:24.781499Z",
     "shell.execute_reply": "2023-03-09T03:07:24.780275Z",
     "shell.execute_reply.started": "2023-03-09T03:07:24.764975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images in Dataset = 8091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_path = \"../Flickr8k_Dataset/Flicker8k_Dataset\"\n",
    "dir_Flickr_text = \"../Flickr8k_text/Flickr8k.token.txt\"\n",
    "features_path = '../Flickr8k_Dataset/features'\n",
    "\n",
    "jpgs = os.listdir(image_path)\n",
    "\n",
    "print(\"Total Images in Dataset = {}\".format(len(jpgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c91490",
   "metadata": {},
   "source": [
    "We create a dataframe to store the image id and captions for ease of use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd70a3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T03:07:24.792695Z",
     "iopub.status.busy": "2023-03-09T03:07:24.791487Z",
     "iopub.status.idle": "2023-03-09T03:07:24.914758Z",
     "shell.execute_reply": "2023-03-09T03:07:24.912894Z",
     "shell.execute_reply.started": "2023-03-09T03:07:24.792666Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>a little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  index                   filename  \\\n",
       "0     0  1000268201_693b08cb0e.jpg   \n",
       "1     1  1000268201_693b08cb0e.jpg   \n",
       "2     2  1000268201_693b08cb0e.jpg   \n",
       "3     3  1000268201_693b08cb0e.jpg   \n",
       "4     4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  a child in a pink dress is climbing up a set o...  \n",
       "1              a girl going into a wooden building .  \n",
       "2   a little girl climbing into a wooden playhouse .  \n",
       "3  a little girl climbing the stairs to her playh...  \n",
       "4  a little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(dir_Flickr_text,'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "datatxt = []\n",
    "for line in text.split('\\n'):\n",
    "   col = line.split('\\t')\n",
    "   if len(col) == 1:\n",
    "       continue\n",
    "   w = col[0].split(\"#\")\n",
    "   datatxt.append(w + [col[1].lower()])\n",
    "\n",
    "data = pd.DataFrame(datatxt,columns=[\"filename\",\"index\",\"caption\"])\n",
    "data = data.reindex(columns =['index','filename','caption'])\n",
    "data = data[data.filename != '2258277193_586949ec62.jpg.1']\n",
    "uni_filenames = np.unique(data.filename.values)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f504f2cf",
   "metadata": {},
   "source": [
    "size of our vocabulary after cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f40bea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T03:07:39.815544Z",
     "iopub.status.busy": "2023-03-09T03:07:39.815097Z",
     "iopub.status.idle": "2023-03-09T03:07:39.847207Z",
     "shell.execute_reply": "2023-03-09T03:07:39.846076Z",
     "shell.execute_reply.started": "2023-03-09T03:07:39.815465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Flickr8k_Dataset/Flicker8k_Dataset/1000268201_693b08cb0e.jpg',\n",
       " '../Flickr8k_Dataset/Flicker8k_Dataset/1000268201_693b08cb0e.jpg',\n",
       " '../Flickr8k_Dataset/Flicker8k_Dataset/1000268201_693b08cb0e.jpg',\n",
       " '../Flickr8k_Dataset/Flicker8k_Dataset/1000268201_693b08cb0e.jpg',\n",
       " '../Flickr8k_Dataset/Flicker8k_Dataset/1000268201_693b08cb0e.jpg',\n",
       " '../Flickr8k_Dataset/Flicker8k_Dataset/1001773457_577c3a7d70.jpg',\n",
       " '../Flickr8k_Dataset/Flicker8k_Dataset/1001773457_577c3a7d70.jpg',\n",
       " '../Flickr8k_Dataset/Flicker8k_Dataset/1001773457_577c3a7d70.jpg',\n",
       " '../Flickr8k_Dataset/Flicker8k_Dataset/1001773457_577c3a7d70.jpg',\n",
       " '../Flickr8k_Dataset/Flicker8k_Dataset/1001773457_577c3a7d70.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_img_name_vector = []\n",
    "for annot in data[\"filename\"]:\n",
    "   full_image_path = image_path + '/' + annot\n",
    "   all_img_name_vector.append(full_image_path)\n",
    "\n",
    "all_img_name_vector[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b3e2c",
   "metadata": {},
   "source": [
    "40455 image paths and captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "484dd616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T03:07:39.850801Z",
     "iopub.status.busy": "2023-03-09T03:07:39.849168Z",
     "iopub.status.idle": "2023-03-09T03:07:39.859687Z",
     "shell.execute_reply": "2023-03-09T03:07:39.858421Z",
     "shell.execute_reply.started": "2023-03-09T03:07:39.850757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_img_name_vector) : 40455\n"
     ]
    }
   ],
   "source": [
    "print(f\"len(all_img_name_vector) : {len(all_img_name_vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c04d71a",
   "metadata": {},
   "source": [
    "Step 3:- Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef020b4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T03:07:39.899069Z",
     "iopub.status.busy": "2023-03-09T03:07:39.898546Z",
     "iopub.status.idle": "2023-03-09T03:07:40.280043Z",
     "shell.execute_reply": "2023-03-09T03:07:40.279041Z",
     "shell.execute_reply.started": "2023-03-09T03:07:39.899040Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, None, 3)]   0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, None, None, 64)    1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, None, None, 64)    36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, None, None, 64)    0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, None, None, 128)   73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, None, None, 128)   147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, None, None, 128)   0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, None, None, 256)   295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, None, None, 256)   590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, None, None, 256)   0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, None, None, 512)   1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, None, None, 512)   2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, None, None, 512)   0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def load_image(image_path):\n",
    "   img = tf.io.read_file(image_path)\n",
    "   img = tf.image.decode_jpeg(img, channels=3)\n",
    "   img = tf.image.resize(img, (224, 224))\n",
    "   img = preprocess_input(img)\n",
    "   return img, image_path\n",
    "\n",
    "image_model = tf.keras.applications.VGG16(include_top=False, weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "image_features_extract_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc2dfaa",
   "metadata": {},
   "source": [
    "Map each image name to the function to load the image:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f9303f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T03:07:40.282035Z",
     "iopub.status.busy": "2023-03-09T03:07:40.281272Z",
     "iopub.status.idle": "2023-03-09T03:07:40.449539Z",
     "shell.execute_reply": "2023-03-09T03:07:40.448457Z",
     "shell.execute_reply.started": "2023-03-09T03:07:40.281993Z"
    }
   },
   "outputs": [],
   "source": [
    "encode_train = sorted(set(all_img_name_vector))\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575d5fc6",
   "metadata": {},
   "source": [
    "extract the features and store them in the respective .npy files and then pass those features through the encoder.NPY files store all the information required to reconstruct an array on any computer, which includes dtype and shape information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80b979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-09T03:07:40.452773Z",
     "iopub.status.busy": "2023-03-09T03:07:40.451284Z",
     "iopub.status.idle": "2023-03-09T03:07:40.457783Z",
     "shell.execute_reply": "2023-03-09T03:07:40.456414Z",
     "shell.execute_reply.started": "2023-03-09T03:07:40.452735Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|████████████████████████████████████████████████████████████████████████████▊ | 125/127 [1:54:44<01:39, 49.87s/it]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for img, path in tqdm(image_dataset):\n",
    " batch_features = image_features_extract_model(img)\n",
    " batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "\n",
    " for bf, p in zip(batch_features, path):\n",
    "   path_of_feature = Path(p.numpy().decode(\"utf-8\"))\n",
    "   image_name = path_of_feature.stem\n",
    "   np.save(f'{features_path}/{image_name}', bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef70d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
